{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Leb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0  -2.9  -2.8  -2.7  -2.6  -2.5  -2.4  -2.3  -2.2  -2.1  -2.0  -1.9  -1.8  -1.7  -1.6  -1.5  -1.4  -1.3  -1.2  -1.1  -1.0  -0.9  -0.8  -0.7  -0.6  -0.5  -0.4  -0.3  -0.2  -0.1  0.0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1.0  1.1  1.2  1.3  1.4  1.5  1.6  1.7  1.8  1.9  2.0  2.1  2.2  2.3  2.4  2.5  2.6  2.7  2.8  2.9  3.0  3.1  3.2  3.3  3.4  3.5  3.6  3.7  3.8  3.9  4.0  4.1  4.2  4.3  4.4  4.5  4.6  4.7  4.8  4.9  "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [1, 2, 3]\n",
    "y = [1, 2, 3]\n",
    "\n",
    "w = tf.placeholder(tf.float32)\n",
    "# b= tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis = x * w\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# append() 할려면 타입 선언먼저 해야함\n",
    "w_val = []\n",
    "cost_val = []\n",
    "\n",
    "# [-30, 50]이 아니라 [-3, 5]로 해버리면 데이터 너무 없어서 직선그래프나옴\n",
    "\"\"\"\n",
    "for i in range(-3, 5):\n",
    "    feed_w = i\n",
    "    curr_cost, curr_w = sess.run([cost, w], feed_dict = {w: feed_w})\n",
    "    w_val.append(curr_w)\n",
    "    cost_val.append(curr_cost)\n",
    "\n",
    "plt.plot(w_val, cost_val)\n",
    "plt.grid()\n",
    "plt.xlabel('w_val')\n",
    "plt.ylabel('cost_val')\n",
    "plt.show()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\"\"\"\n",
    "for i in range(-30, 50):\n",
    "    feed_w = i * 0.1\n",
    "    curr_cost, curr_w = sess.run([cost, w], feed_dict = {w: feed_w})\n",
    "    print(curr_w ,\" \", end=\"\")\n",
    "    w_val.append(curr_w)\n",
    "    cost_val.append(curr_cost)\n",
    "\n",
    "plt.plot(w_val, cost_val)\n",
    "plt.grid()\n",
    "plt.xlabel('w_val')\n",
    "plt.ylabel('cost_val')\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w 값 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.17307222 [0.8074204]\n",
      "200 0.0 [1.]\n",
      "400 0.0 [1.]\n",
      "600 0.0 [1.]\n",
      "800 0.0 [1.]\n",
      "1000 0.0 [1.]\n",
      "1200 0.0 [1.]\n",
      "1400 0.0 [1.]\n",
      "1600 0.0 [1.]\n",
      "1800 0.0 [1.]\n",
      "2000 0.0 [1.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "w = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "\n",
    "hypothesis = x * w\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis -y))\n",
    "\n",
    "\n",
    "##### w 값 조정#####\n",
    "# cost minimize 값 우리가 정의함\n",
    "\n",
    "learning_rate = 0.1\n",
    "gradient = tf.reduce_mean((w * x - y)*x)\n",
    "descent = w - learning_rate * gradient\n",
    "update = w.assign(descent)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# update 반복문안에 넣음\n",
    "for step in range(2001):\n",
    "    sess.run(update, feed_dict = {x: [1, 2, 3], y: [1, 2, 3]})\n",
    "    if step % 200 == 0:\n",
    "        print(step, sess.run(cost, feed_dict={x: [1, 2, 3], y:[1, 2, 3]}),\n",
    "              sess.run(w))\n",
    "            \n",
    "       \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gradient 값 조정\n",
    "\n",
    "gvs 값 바꿔보면 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [18.666666, 5.0, [(37.333336, 5.0)]]\n",
      "1 [16.924444, 4.6266665, [(33.84889, 4.6266665)]]\n",
      "2 [15.344829, 4.2881775, [(30.689657, 4.2881775)]]\n",
      "3 [13.912643, 3.9812808, [(27.825287, 3.9812808)]]\n",
      "4 [12.614131, 3.703028, [(25.228264, 3.703028)]]\n",
      "5 [11.4368105, 3.4507453, [(22.873623, 3.4507453)]]\n",
      "6 [10.369376, 3.2220092, [(20.738752, 3.2220092)]]\n",
      "7 [9.401568, 3.0146217, [(18.803137, 3.0146217)]]\n",
      "8 [8.524088, 2.8265903, [(17.048176, 2.8265903)]]\n",
      "9 [7.7285066, 2.6561086, [(15.457014, 2.6561086)]]\n",
      "10 [7.0071797, 2.5015385, [(14.01436, 2.5015385)]]\n",
      "11 [6.353176, 2.361395, [(12.706352, 2.361395)]]\n",
      "12 [5.7602134, 2.2343314, [(11.520427, 2.2343314)]]\n",
      "13 [5.222593, 2.119127, [(10.445186, 2.119127)]]\n",
      "14 [4.735151, 2.0146751, [(9.470302, 2.0146751)]]\n",
      "15 [4.2932034, 1.9199722, [(8.586407, 1.9199722)]]\n",
      "16 [3.8925045, 1.8341081, [(7.785009, 1.8341081)]]\n",
      "17 [3.5292041, 1.756258, [(7.0584083, 1.756258)]]\n",
      "18 [3.199812, 1.685674, [(6.399624, 1.685674)]]\n",
      "19 [2.9011629, 1.6216778, [(5.8023257, 1.6216778)]]\n",
      "20 [2.630388, 1.5636545, [(5.260776, 1.5636545)]]\n",
      "21 [2.3848848, 1.5110468, [(4.7697697, 1.5110468)]]\n",
      "22 [2.1622956, 1.4633491, [(4.324591, 1.4633491)]]\n",
      "23 [1.9604816, 1.4201032, [(3.9209635, 1.4201032)]]\n",
      "24 [1.7775034, 1.3808936, [(3.5550067, 1.3808936)]]\n",
      "25 [1.6116028, 1.3453435, [(3.2232056, 1.3453435)]]\n",
      "26 [1.4611868, 1.3131114, [(2.9223738, 1.3131114)]]\n",
      "27 [1.3248094, 1.2838877, [(2.6496186, 1.2838877)]]\n",
      "28 [1.2011608, 1.2573916, [(2.4023218, 1.2573916)]]\n",
      "29 [1.0890526, 1.2333684, [(2.178105, 1.2333684)]]\n",
      "30 [0.9874074, 1.2115873, [(1.9748147, 1.2115873)]]\n",
      "31 [0.89524966, 1.1918392, [(1.7904994, 1.1918392)]]\n",
      "32 [0.811693, 1.1739342, [(1.6233861, 1.1739342)]]\n",
      "33 [0.73593473, 1.1577003, [(1.4718695, 1.1577003)]]\n",
      "34 [0.6672478, 1.1429816, [(1.3344957, 1.1429816)]]\n",
      "35 [0.6049709, 1.1296366, [(1.2099419, 1.1296366)]]\n",
      "36 [0.5485072, 1.1175373, [(1.0970144, 1.1175373)]]\n",
      "37 [0.49731335, 1.1065671, [(0.9946267, 1.1065671)]]\n",
      "38 [0.45089749, 1.0966209, [(0.901795, 1.0966209)]]\n",
      "39 [0.40881374, 1.087603, [(0.81762755, 1.087603)]]\n",
      "40 [0.37065756, 1.0794266, [(0.7413151, 1.0794266)]]\n",
      "41 [0.33606312, 1.0720135, [(0.6721263, 1.0720135)]]\n",
      "42 [0.304697, 1.0652922, [(0.609394, 1.0652922)]]\n",
      "43 [0.27625844, 1.0591983, [(0.5525169, 1.0591983)]]\n",
      "44 [0.25047457, 1.0536731, [(0.50094914, 1.0536731)]]\n",
      "45 [0.22709687, 1.0486636, [(0.45419377, 1.0486636)]]\n",
      "46 [0.20590079, 1.0441216, [(0.41180158, 1.0441216)]]\n",
      "47 [0.18668361, 1.0400037, [(0.37336725, 1.0400037)]]\n",
      "48 [0.16925998, 1.03627, [(0.33852, 1.03627)]]\n",
      "49 [0.15346257, 1.0328848, [(0.30692515, 1.0328848)]]\n",
      "50 [0.13913913, 1.0298156, [(0.2782783, 1.0298156)]]\n",
      "51 [0.12615263, 1.0270327, [(0.25230527, 1.0270327)]]\n",
      "52 [0.11437845, 1.0245097, [(0.2287569, 1.0245097)]]\n",
      "53 [0.103702866, 1.022222, [(0.20740573, 1.022222)]]\n",
      "54 [0.09402418, 1.020148, [(0.18804836, 1.020148)]]\n",
      "55 [0.08524827, 1.0182675, [(0.17049655, 1.0182675)]]\n",
      "56 [0.07729217, 1.0165626, [(0.15458435, 1.0165626)]]\n",
      "57 [0.07007837, 1.0150168, [(0.14015675, 1.0150168)]]\n",
      "58 [0.063537955, 1.0136153, [(0.12707591, 1.0136153)]]\n",
      "59 [0.05760769, 1.0123445, [(0.11521538, 1.0123445)]]\n",
      "60 [0.052230835, 1.0111923, [(0.10446167, 1.0111923)]]\n",
      "61 [0.04735601, 1.0101477, [(0.09471202, 1.0101477)]]\n",
      "62 [0.04293601, 1.0092006, [(0.08587202, 1.0092006)]]\n",
      "63 [0.038929027, 1.0083419, [(0.07785805, 1.0083419)]]\n",
      "64 [0.035295647, 1.0075634, [(0.07059129, 1.0075634)]]\n",
      "65 [0.03200118, 1.0068574, [(0.06400236, 1.0068574)]]\n",
      "66 [0.02901423, 1.0062174, [(0.05802846, 1.0062174)]]\n",
      "67 [0.026306113, 1.005637, [(0.052612226, 1.005637)]]\n",
      "68 [0.023851236, 1.005111, [(0.047702473, 1.005111)]]\n",
      "69 [0.021624884, 1.0046339, [(0.043249767, 1.0046339)]]\n",
      "70 [0.01960659, 1.0042014, [(0.03921318, 1.0042014)]]\n",
      "71 [0.017776767, 1.0038093, [(0.035553537, 1.0038093)]]\n",
      "72 [0.016118089, 1.0034539, [(0.03223618, 1.0034539)]]\n",
      "73 [0.014613827, 1.0031315, [(0.029227655, 1.0031315)]]\n",
      "74 [0.013249755, 1.0028392, [(0.02649951, 1.0028392)]]\n",
      "75 [0.0120129585, 1.0025742, [(0.024025917, 1.0025742)]]\n",
      "76 [0.010891874, 1.002334, [(0.02178375, 1.002334)]]\n",
      "77 [0.009875615, 1.0021162, [(0.019751232, 1.0021162)]]\n",
      "78 [0.0089536905, 1.0019187, [(0.017907381, 1.0019187)]]\n",
      "79 [0.008118351, 1.0017396, [(0.016236704, 1.0017396)]]\n",
      "80 [0.007360419, 1.0015773, [(0.014720838, 1.0015773)]]\n",
      "81 [0.006673495, 1.00143, [(0.013346991, 1.00143)]]\n",
      "82 [0.006050428, 1.0012965, [(0.012100856, 1.0012965)]]\n",
      "83 [0.0054858923, 1.0011755, [(0.010971785, 1.0011755)]]\n",
      "84 [0.0049740872, 1.0010659, [(0.009948175, 1.0010659)]]\n",
      "85 [0.004509449, 1.0009663, [(0.009018898, 1.0009663)]]\n",
      "86 [0.0040884414, 1.0008761, [(0.008176884, 1.0008761)]]\n",
      "87 [0.0037065744, 1.0007943, [(0.007413149, 1.0007943)]]\n",
      "88 [0.003360788, 1.0007201, [(0.006721576, 1.0007201)]]\n",
      "89 [0.0030470293, 1.0006529, [(0.0060940585, 1.0006529)]]\n",
      "90 [0.0027626355, 1.000592, [(0.0055252714, 1.000592)]]\n",
      "91 [0.0025049448, 1.0005368, [(0.0050098896, 1.0005368)]]\n",
      "92 [0.0022712946, 1.0004867, [(0.004542589, 1.0004867)]]\n",
      "93 [0.0020594597, 1.0004413, [(0.0041189194, 1.0004413)]]\n",
      "94 [0.0018669764, 1.0004001, [(0.003733953, 1.0004001)]]\n",
      "95 [0.0016927322, 1.0003628, [(0.0033854644, 1.0003628)]]\n",
      "96 [0.0015347401, 1.0003289, [(0.0030694804, 1.0003289)]]\n",
      "97 [0.0013918877, 1.0002983, [(0.0027837753, 1.0002983)]]\n",
      "98 [0.0012617111, 1.0002704, [(0.0025234222, 1.0002704)]]\n",
      "99 [0.0011437734, 1.0002451, [(0.0022875469, 1.0002451)]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = [1, 2, 3]\n",
    "y = [1, 2, 3]\n",
    "\n",
    "#x = tf.placeholder(tf.float32)\n",
    "#y = tf.placeholder(tf.float32)\n",
    "\n",
    "#높은 값 5.0으로 둬봄\n",
    "w = tf.Variable(5.)\n",
    "\n",
    "hypothesis = x * w\n",
    "\n",
    "# 우리가 짠 gradient\n",
    "# gradient = tf.reduce_mean((w * x -y) * x) * 2\n",
    "gradient = tf.reduce_mean((w * x -y) * x)\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# 원래 있는 gradient 계산\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "apply_gradients = optimizer.apply_gradients(gvs)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# apply_gradient 반복문안에 넣음\n",
    "for step in range(100):\n",
    "    print(step, sess.run([gradient, w, gvs]))    \n",
    "    sess.run(apply_gradients)\n",
    "    \n",
    "# gradient = 0\n",
    "# w = 1\n",
    "# gvs = [gradient, w]\n",
    "\n",
    "# Q. gradient가 왜 1이아니라 0\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
